{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_PATH: C:\\Users\\LAPTOP\\Graduation-Project\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.gitignore',\n",
       " '.vscode',\n",
       " 'chatbot-env',\n",
       " 'DataEngineering',\n",
       " 'FineTuneing',\n",
       " 'hierarchy.txt',\n",
       " 'README.md',\n",
       " 'requirements.txt',\n",
       " 'Terminal.ipynb',\n",
       " 'Testing Interface.ipynb',\n",
       " 'Utils',\n",
       " 'WebSocket Interface']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title # Setting up the environment { vertical-output: true, display-mode: \"form\" }\n",
    "\n",
    "###################\n",
    "#####  SETUP  #####\n",
    "###################\n",
    "\n",
    "#@title Setting up project paths\n",
    "import os\n",
    "\n",
    "colab_setup = False #@param {type:\"boolean\"}\n",
    "PROJECT_PATH = \"/content/drive/MyDrive/TWM/Graduation-Project/\" #@param {\"type\":\"string\"}\n",
    "\n",
    "if colab_setup:\n",
    "    from google.colab import drive\n",
    "    print(\"Mounting Google Drive...\", end=\"\", flush=True)\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Done\")\n",
    "\n",
    "else:\n",
    "    # set this to the parent directory of the whole project\n",
    "    PROJECT_PATH = rf\"C:\\Users\\{os.environ['USERNAME']}\\Graduation-Project\"\n",
    "\n",
    "print(\"PROJECT_PATH:\", PROJECT_PATH)\n",
    "os.chdir(PROJECT_PATH)\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.EasyT5 import EasyT5\n",
    "import multiprocessing\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from transformers import T5TokenizerFast as T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"DataEngineering/FinalDataset/large/\" #@param {\"type\": \"string\"}\n",
    "MODEL_CHECKPOINTS = \"FineTuning/T5/checkpoints/\" #@param {\"type\": \"string\"}\n",
    "TENSORBOARD_LOGS = \"FineTuning/T5/TB_LOGS/\" #@param {\"type\":\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = EasyT5.ExperimentParameters()\n",
    "\n",
    "SEED = 512\n",
    "\n",
    "# parameters related to the training process\n",
    "# and the PyTorch Lightning trainer\n",
    "parameters['trainer'] = {\n",
    "    # saves the last recent 'n' epochs\n",
    "    \"save_last_n_epochs\": 3,\n",
    "    # the fixed learning rate for the model\n",
    "    \"fixed_learning_rate\": 1e-4,\n",
    "    # the monitor of the early stopping\n",
    "    \"early_stopping_monitor\": \"val_loss\",\n",
    "    # the minimum delta between the epochs to apply early stopping\n",
    "    \"early_stopping_min_delta\": 0.01,\n",
    "    # 0 to disable early stopping feature\n",
    "    \"early_stopping_patience_epochs\": 0,\n",
    "    # the mode of the early stopping criteria\n",
    "    \"early_stopping_mode\": \"min\",\n",
    "    # the maximum number of epochs to train/fine-tune the model on\n",
    "    \"max_epochs\": 5,\n",
    "    # the floating point numbers precision\n",
    "    \"precision\": 32,\n",
    "    # the training batch size \n",
    "    # the batch size at which the data is loaded into memory\n",
    "    \"batch_size\": 8,\n",
    "}\n",
    "\n",
    "# general parameters about the working environment\n",
    "parameters['general'] = {\n",
    "    # the output directory\n",
    "    \"output_dir\":\"\",\n",
    "    # the name/path of the checkpoint to be loaded from Hugging face\n",
    "    # or from the local disk\n",
    "    \"checkpoint_name\": MODEL_CHECKPOINTS+\"-epoch-9-tloss-1.5577-vloss-1.8296\",\n",
    "    # the name that will appear on tensorboard\n",
    "    \"tensorboard_name\": \"t5-v1_1-base_BatchSize-16_N-Splits-4_DatasetSize-large_Topic-Food&Drink_version_2\",\n",
    "    # the number of cpu cores in the current machine\n",
    "    \"cpu_cores\": multiprocessing.cpu_count(),\n",
    "    # the environment seed\n",
    "    'seed':SEED,\n",
    "}\n",
    "\n",
    "# the parameters passed to the tokenizer when encoding text\n",
    "parameters['encoder'] = {\n",
    "    # the padding method for the input sequences\n",
    "    \"padding\":\"max_length\",\n",
    "    # whether to truncate long sequences or not\n",
    "    \"truncation\":True,\n",
    "    # whether to add special tokens in the input sequences or not\n",
    "    \"add_special_tokens\": True,\n",
    "    # the maximum length of the input sequence\n",
    "    \"max_length\": 512,\n",
    "}\n",
    "\n",
    "# the parameters passed to the model when generating text\n",
    "parameters['generator'] = {\n",
    "    # the number of beams used in the beam search (also known as beam width)\n",
    "    \"num_beams\": 2,\n",
    "    # the maximum length of the generated sequences\n",
    "    \"max_length\": 512,\n",
    "    # the repetition penalty added when the model repeats words\n",
    "    \"repetition_penalty\": 2.5,\n",
    "    # the penalty aded when the model generates lengthly sequences\n",
    "    \"length_penalty\": 1.0,\n",
    "    # whether to stop the beam search when at least `num_beams` sentences are finished per batch or not.\n",
    "    \"early_stopping\": True,\n",
    "    # if set to float < 1, only the most probable tokens with probabilities that add up to `top_p` or higher are kept for generation.\n",
    "    \"top_p\": 0.95,\n",
    "    # the number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "    \"top_k\": 50,\n",
    "    # the number of returned sequences\n",
    "    \"num_return_sequences\": 1,\n",
    "    # whether to skip special tokens when generating or not\n",
    "    \"skip_special_tokens\": True,\n",
    "    # whether to clean all tokenization spaces before returning the output or not\n",
    "    \"clean_up_tokenization_spaces\": True,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"num_return_sequences\": 3, \n",
    "    \"num_beams\": 3, \n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 100,\n",
    "    \"temperature\": 1.0 \n",
    "}\n",
    "\n",
    "def get_suggestions(model, prompt, return_scores=False, kwargs=kwargs):\n",
    "    # suggestions, scores = model.predict(\"complete: \" + prompt, **kwargs)\n",
    "    suggestions, scores = model.predict(\"complete: \" + prompt)#, **kwargs)\n",
    "    if return_scores:\n",
    "        return suggestions, scores\n",
    "\n",
    "    print(\"\\n\".join(suggestions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "There was a specific connection error when trying to load ('FineTuning/T5/checkpoints/-epoch-9-tloss-1.5577-vloss-1.8296',):\n404 Client Error: Not Found for url: https://huggingface.co/('FineTuning/T5/checkpoints/-epoch-9-tloss-1.5577-vloss-1.8296',)/resolve/main/config.json (Request ID: 6G_kGBHctOcag7o4c5qtA)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\LAPTOP\\Graduation-Project\\chatbot-env\\lib\\site-packages\\transformers\\utils\\hub.py:408\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    407\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 408\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    409\u001b[0m         path_or_repo_id,\n\u001b[0;32m    410\u001b[0m         filename,\n\u001b[0;32m    411\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    412\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    413\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    414\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    415\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    416\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    417\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    418\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    419\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    420\u001b[0m     )\n\u001b[0;32m    422\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP\\Graduation-Project\\chatbot-env\\lib\\site-packages\\huggingface_hub\\file_download.py:1099\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, use_auth_token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m     _raise_for_status(r)\n\u001b[0;32m   1100\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP\\Graduation-Project\\chatbot-env\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:169\u001b[0m, in \u001b[0;36m_raise_for_status\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    167\u001b[0m _add_server_message_to_error_args(e, response)\n\u001b[1;32m--> 169\u001b[0m \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP\\Graduation-Project\\chatbot-env\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:131\u001b[0m, in \u001b[0;36m_raise_for_status\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 131\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[0;32m    132\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP\\Graduation-Project\\chatbot-env\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/('FineTuning/T5/checkpoints/-epoch-9-tloss-1.5577-vloss-1.8296',)/resolve/main/config.json (Request ID: 6G_kGBHctOcag7o4c5qtA)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39m# load the model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model \u001b[39m=\u001b[39m EasyT5(parameters)\n\u001b[1;32m----> 6\u001b[0m model\u001b[39m.\u001b[39;49mfrom_pretrained(T5ForConditionalGeneration, T5Tokenizer, return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32m~\\Graduation-Project\\Utils\\EasyT5.py:387\u001b[0m, in \u001b[0;36mEasyT5.from_pretrained\u001b[1;34m(self, tokenizer_class, model_class, return_dict, use_gpu, fp16)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\n\u001b[0;32m    378\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    379\u001b[0m     tokenizer_class: PreTrainedTokenizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     use_gpu: \u001b[39mbool\u001b[39m\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m     fp16: \u001b[39mbool\u001b[39m\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[39m        loads a model (from a local folder or from HF) for training/fine-tuning/evaluating/inference\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m tokenizer_class\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheckpoint_name)\n\u001b[0;32m    388\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    389\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoint_name, \n\u001b[0;32m    390\u001b[0m         return_dict\u001b[39m=\u001b[39mreturn_dict \u001b[39m# set this to false during inference\u001b[39;00m\n\u001b[0;32m    391\u001b[0m     )\n\u001b[0;32m    393\u001b[0m     \u001b[39mif\u001b[39;00m use_gpu:\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP\\Graduation-Project\\chatbot-env\\lib\\site-packages\\transformers\\modeling_utils.py:1918\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m   1917\u001b[0m     config_path \u001b[39m=\u001b[39m config \u001b[39mif\u001b[39;00m config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[1;32m-> 1918\u001b[0m     config, model_kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mconfig_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m   1919\u001b[0m         config_path,\n\u001b[0;32m   1920\u001b[0m         cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   1921\u001b[0m         return_unused_kwargs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   1922\u001b[0m         force_download\u001b[39m=\u001b[39mforce_download,\n\u001b[0;32m   1923\u001b[0m         resume_download\u001b[39m=\u001b[39mresume_download,\n\u001b[0;32m   1924\u001b[0m         proxies\u001b[39m=\u001b[39mproxies,\n\u001b[0;32m   1925\u001b[0m         local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1926\u001b[0m         use_auth_token\u001b[39m=\u001b[39muse_auth_token,\n\u001b[0;32m   1927\u001b[0m         revision\u001b[39m=\u001b[39mrevision,\n\u001b[0;32m   1928\u001b[0m         subfolder\u001b[39m=\u001b[39msubfolder,\n\u001b[0;32m   1929\u001b[0m         _from_auto\u001b[39m=\u001b[39mfrom_auto_class,\n\u001b[0;32m   1930\u001b[0m         _from_pipeline\u001b[39m=\u001b[39mfrom_pipeline,\n\u001b[0;32m   1931\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1932\u001b[0m     )\n\u001b[0;32m   1933\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1934\u001b[0m     model_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP\\Graduation-Project\\chatbot-env\\lib\\site-packages\\transformers\\configuration_utils.py:526\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    450\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mcls\u001b[39m, pretrained_model_name_or_path: Union[\u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPretrainedConfig\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    451\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[39m    Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[39m    assert unused_kwargs == {\"foo\": False}\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[39m    ```\"\"\"\u001b[39;00m\n\u001b[1;32m--> 526\u001b[0m     config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    527\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type:\n\u001b[0;32m    528\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    529\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou are using a model of type \u001b[39m\u001b[39m{\u001b[39;00mconfig_dict[\u001b[39m'\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m to instantiate a model of type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    530\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    531\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP\\Graduation-Project\\chatbot-env\\lib\\site-packages\\transformers\\configuration_utils.py:553\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    552\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 553\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    554\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[0;32m    555\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP\\Graduation-Project\\chatbot-env\\lib\\site-packages\\transformers\\configuration_utils.py:608\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    604\u001b[0m configuration_file \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_configuration_file\u001b[39m\u001b[39m\"\u001b[39m, CONFIG_NAME)\n\u001b[0;32m    606\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    607\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[0;32m    609\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m    610\u001b[0m         configuration_file,\n\u001b[0;32m    611\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    612\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    613\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    614\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    615\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    616\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    617\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    618\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    619\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[0;32m    620\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[0;32m    621\u001b[0m     )\n\u001b[0;32m    622\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m    623\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[0;32m    624\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[0;32m    625\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP\\Graduation-Project\\chatbot-env\\lib\\site-packages\\transformers\\utils\\hub.py:465\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _raise_exceptions_for_connection_errors:\n\u001b[0;32m    463\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 465\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThere was a specific connection error when trying to load \u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00merr\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    467\u001b[0m \u001b[39mreturn\u001b[39;00m resolved_file\n",
      "\u001b[1;31mOSError\u001b[0m: There was a specific connection error when trying to load ('FineTuning/T5/checkpoints/-epoch-9-tloss-1.5577-vloss-1.8296',):\n404 Client Error: Not Found for url: https://huggingface.co/('FineTuning/T5/checkpoints/-epoch-9-tloss-1.5577-vloss-1.8296',)/resolve/main/config.json (Request ID: 6G_kGBHctOcag7o4c5qtA)"
     ]
    }
   ],
   "source": [
    "parameters['general'][\"checkpoint_name\"] = MODEL_CHECKPOINTS+\"-epoch-9-tloss-1.5577-vloss-1.8296\",\n",
    "parameters['general'][\"tensorboard_name\"] = \"t5-v1_1-base_BatchSize-16_N-Splits-4_DatasetSize-large_Topic-Food&Drink_version_2\",\n",
    "\n",
    "# load the model\n",
    "model = EasyT5(parameters)\n",
    "model.from_pretrained(T5ForConditionalGeneration, T5Tokenizer, return_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'FineTuning/T5/checkpoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m os\u001b[39m.\u001b[39;49mlistdir(\u001b[39m\"\u001b[39;49m\u001b[39mFineTuning/T5/checkpoints\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'FineTuning/T5/checkpoints'"
     ]
    }
   ],
   "source": [
    "os.listdir(\"FineTuning/T5/checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "suggestion = model.predict(\"complete: I want to eat\")[0]\n",
    "print(suggestion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('chatbot-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "03d17e92b9af9393008ccb7443bbe96b4789aebfc0fb054f8d2c26b133cdc647"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
